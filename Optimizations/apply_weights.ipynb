{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CLIP-HBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV with original columns saved to /home/wallacelab/complexity-final/Embeddings/CLIP-HBA/THINGS/crossval_optimized_scaled_things_hba_embedding.csv\n",
      "Predicted Complexity Scores saved to /home/wallacelab/complexity-final/Embeddings/CLIP-HBA/THINGS/crossval_things_hba_scaled_predicted_complexity_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files\n",
    "first_csv = pd.read_csv('/home/wallacelab/complexity-final/Embeddings/CLIP-HBA/THINGS/scaled_things_hba_embedding.csv')\n",
    "second_csv = pd.read_csv('/home/wallacelab/complexity-final/Optimizations/BestWeights/HBA/THINGS/things/crossval_final_weights_scaled_hba_complexity_prediction.csv')\n",
    "\n",
    "# Ensure column names in the first CSV are strings for proper matching\n",
    "first_csv.columns = first_csv.columns.map(str)\n",
    "\n",
    "# Create a dictionary to map 'Predictor' (converted to string) to 'Mean Weight'\n",
    "predictor_weights = dict(zip(second_csv['Predictor'].astype(str), second_csv['Mean Weight']))\n",
    "\n",
    "# List of predictors (columns to update), assumed to be numbered 0 through 48\n",
    "columns_to_update = [str(i) for i in range(49)]  # Convert to strings to match column names\n",
    "\n",
    "# Multiply each column in the first CSV by the corresponding weight\n",
    "for column in columns_to_update:\n",
    "    if column in first_csv.columns and column in predictor_weights:\n",
    "        first_csv[column] *= predictor_weights[column]\n",
    "\n",
    "# Calculate the Predicted Complexity Score by summing the weighted columns\n",
    "first_csv['Predicted Complexity Score'] = first_csv[columns_to_update].sum(axis=1)\n",
    "\n",
    "# Save the first CSV with original and weighted columns\n",
    "first_output_path = '/home/wallacelab/complexity-final/Embeddings/CLIP-HBA/THINGS/crossval_optimized_scaled_things_hba_embedding.csv'\n",
    "first_csv.to_csv(first_output_path, index=False)\n",
    "\n",
    "# Save the second CSV with just the Predicted Complexity Score\n",
    "second_output_path = '/home/wallacelab/complexity-final/Embeddings/CLIP-HBA/THINGS/crossval_things_hba_scaled_predicted_complexity_scores.csv'\n",
    "first_csv[['Predicted Complexity Score']].to_csv(second_output_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV with original columns saved to {first_output_path}\")\n",
    "print(f\"Predicted Complexity Scores saved to {second_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled CSV saved to /home/wallacelab/complexity-final/Embeddings/CLIP-HBA/THINGS/scaled_things_hba_embedding.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/home/wallacelab/complexity-final/Embeddings/CLIP-HBA/THINGS/things_hba_embedding.csv'  # Replace with the path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the integer values row-wise\n",
    "for index, row in df.iterrows():\n",
    "    # Select only numeric columns for scaling (excluding the 'image' column)\n",
    "    numeric_values = row.drop(labels=['image'])\n",
    "    scaled_values = scaler.fit_transform(numeric_values.values.reshape(-1, 1)).flatten()\n",
    "    df.loc[index, numeric_values.index] = scaled_values\n",
    "\n",
    "# Save the scaled DataFrame to a new CSV file\n",
    "output_file_path = '/home/wallacelab/complexity-final/Embeddings/CLIP-HBA/THINGS/scaled_things_hba_embedding.csv'  # Replace with the desired output file path\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Scaled CSV saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CLIP-CBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV with original columns saved to /home/wallacelab/complexity-final/Embeddings/CLIP-CBA/THINGS/crossval_optimized_things_cba_embedding.csv\n",
      "Predicted Complexity Scores saved to /home/wallacelab/complexity-final/Embeddings/CLIP-CBA/THINGS/crossval_cba_predicted_things_complexity_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files\n",
    "first_csv = pd.read_csv('/home/wallacelab/complexity-final/Embeddings/CLIP-CBA/THINGS/things_cba_embedding.csv')\n",
    "second_csv = pd.read_csv('/home/wallacelab/complexity-final/Optimizations/BestWeights/CBA/THINGS/crossval_final_weights_complexity_prediction.csv')\n",
    "\n",
    "# Ensure column names in the first CSV are strings for proper matching\n",
    "first_csv.columns = first_csv.columns.map(str)\n",
    "\n",
    "# Create a dictionary to map 'Predictor' (converted to string) to 'Mean Weight'\n",
    "predictor_weights = dict(zip(second_csv['Predictor'].astype(str), second_csv['Mean Weight']))\n",
    "\n",
    "# List of predictors (columns to update)\n",
    "columns_to_update = [\n",
    "    \"Irregularity\", \"Disorganized\", \"Asymmetry\", \"Chaotic\", \"Randomness\",\n",
    "    \"Variability\", \"Multicolored\", \"Heterogeneity\", \"Grainy\", \"Isotropy\",\n",
    "    \"Cluttered\", \"Ambiguity\", \"Intricate\"\n",
    "]\n",
    "\n",
    "# Multiply each column in the first CSV by the corresponding weight\n",
    "for column in columns_to_update:\n",
    "    if column in first_csv.columns and column in predictor_weights:\n",
    "        first_csv[column] *= predictor_weights[column]\n",
    "\n",
    "# Calculate the Predicted Complexity Score by summing the weighted columns\n",
    "first_csv['Predicted Complexity Score'] = first_csv[columns_to_update].sum(axis=1)\n",
    "\n",
    "# Save the first CSV with original and weighted columns\n",
    "first_output_path = '/home/wallacelab/complexity-final/Embeddings/CLIP-CBA/THINGS/crossval_optimized_things_cba_embedding.csv'\n",
    "first_csv.to_csv(first_output_path, index=False)\n",
    "\n",
    "# Save the second CSV with just the Predicted Complexity Score\n",
    "second_output_path = '/home/wallacelab/complexity-final/Embeddings/CLIP-CBA/THINGS/crossval_cba_predicted_things_complexity_scores.csv'\n",
    "first_csv[['Predicted Complexity Score']].to_csv(second_output_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV with original columns saved to {first_output_path}\")\n",
    "print(f\"Predicted Complexity Scores saved to {second_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complexity_experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
